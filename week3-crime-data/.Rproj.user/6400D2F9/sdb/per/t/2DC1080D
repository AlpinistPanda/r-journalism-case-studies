{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Investigating homicide statistics\"\nauthor: \"Andrew Ba Tran\"\ndate: \"1/29/2017\"\noutput: html_document\n---\n\nImporting different data sets, calculations\n\nGoal: Dealing with Excel files, analyzing data, adjusting for population.\n\nDonald Trump promised to save America from the hellish wave of crime and disorder in his inaugural address.\n\n\"The crime and the gangs and the drugs that have stolen too many lives and robbed our country of so much unrealized potential,\" said President Trump. \"This American carnage stops right here and stops right now.\"\n\nA statement on the White House website lays out the foundation for their claims.\n\n![White House statement on crime](images/trump1.png)\n\nThe Trump adminstration alleges that\n\n* In 2015, homicides increased by 17% in America’s fifty largest cities\n* That’s the largest increase in 25 years\n* In our nation’s capital, killings rose by 50 percent over the past four years\n* There were thousands of shootings in Chicago last year alone\n\nTrump does not have a great track record when it comes to [accurate representation](https://www.washingtonpost.com/graphics/politics/2016-election/trump-charts/) of data.\n\nSo think like a journalist. \n\nIs this true? How can it be verified? \n\nWith data on homicides. Where can we get that data?\n\nReframe that question.\n\nWho would collect data on homicides and why? (Think local then go wider.)\n\n* Police departments\n    + To arrest those responsible\n* City governments\n    + To track population, public policy adjustments\n* Local news media\n    + To report on events that affect the public\n* The Justice Department\n    + To track local, regional, state trends\n* The Federal Bureau of Investigation\n    + To track local statistics\n* Insurance, real estate companies, businesses\n    + To measure risk and likelihood of crime in an area to set rates\n\nHow can you get the data from them?\n\n* Call them up\n* Check online\n* File a FOIA request\n\nLet's use the FBI's Uniform Crime Reporting Statistics\n\nThey've collected data from police departments across the country for decades.\n\nEach year they create a new report. However, their overall [table builder tool](https://www.ucrdatatool.gov/) that lets people pull data across several years only goes up to 2014. It doesn't have the latest 2015 data.\n\nBut that's alright for now.\n\nTo verify this first claim \"In 2015, homicides increased by 17% in America’s fifty largest cities\" we need the 2015 and 2014 data to compare to each other.\n\nWe'll need a list of the 50 largest cities and we'll need homicide totals from 2014 and 2015 to measure the percent change.\n\nThe 2015 data is [here](https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015) and the 2014 data is [here](https://ucr.fbi.gov/crime-in-the-u.s/2014/crime-in-the-u.s.-2014).\n\nIt's not the most intuitive site to navigate, so I'll point you in the right direction.\n\nClick the 'Download Excel' sheet on [Table 8](https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-8/table_8_offenses_known_to_law_enforcement_by_state_by_city_2015.xls/view) of the 2015 data and save it to your **data** folder.\n\nDo the same for the 2014 data on [Table 8](https://ucr.fbi.gov/crime-in-the-u.s/2014/crime-in-the-u.s.-2014/tables/table-8/Table_8_Offenses_Known_to_Law_Enforcement_by_State_by_City_2014.xls/view).\n\n![You should now have two Excel spreadsheets](images/fbi1.png)\n\nLet's load them in and see what they look like.\n\nWe'll use the [**readxl**](https://github.com/hadley/readxl) package.\n\n```{r reading_data, message=F, warning=F}\nlibrary(readxl)\nh2014 <- read_excel(\"data/table-8.xls\", sheet=1)\n```\n\n\n![This is what it looks when you **View()** the dataframe in RStudio.](images/fbi2.png)\n\nProblems, right? \n\n![That's because it reflects the formating of the Excel spreadsheet.](images/fbi3.png)\n\nIt's not the tidy data structure we're used to working with in R.\n\nThere are too many cells with NAs.\n\nWe have to clean it up.\n\n```{r reading_data2, message=F, warning=F}\n#Fortunately, we can tell the read_excel() function to skip to a certain row when bringing in data\n\nh2014 <- read_excel(\"data/table-8.xls\", sheet=1, skip=3)\n```\n\n![Better.](images/fbi4.png)\nThe columns have been named correctly, but it's good practice to change column names so they're syntactically valid. This means no spaces or odd characters.\n\nTo do that, use the **make.names()** function.\n\n```{r reading_data3}\ncolnames(h2014) <- make.names(colnames(h2014))\n```\n\n![Now, were talking.](images/fbi5.png)\n\nBut now we need to replace the `NA`s in the **State** column.\n\nThere might be a quicker way to do it, but for now, I'm going to show you a quick loop to do it.\n\n```{r reading_data4}\n# What this loop does\n\n# For every row between 1 and the the total number of rows in h2014 dataframe\nfor (i in 1:nrow(h2014)) {\n  \n  # If the cell in State is blank,\n  if(is.na(h2014$State[i])) {\n    # Then fill it with the value of the State cell previous to it\n    h2014$State[i]<-h2014$State[i-1]\n  } \n}\n\n#This way the cell will be filled with the last value, which logically, is the state identifier.\n```\n\n![Excellent.](images/fbi6.png)\n\nNow, let's do this to the 2015 data by judge adjusting the code from above.\n\n```{r reading_data5}\nh2015 <- read_excel(\"data/Table_8_Offenses_Known_to_Law_Enforcement_by_State_by_City_2015.xls\", sheet=1, skip=3)\n\ncolnames(h2015) <- make.names(colnames(h2015))\n\n# For every row between 1 and the the total number of rows in h2015 dataframe\nfor (i in 1:nrow(h2015)) {\n  \n  # If the cell in State is blank,\n  if(is.na(h2015$State[i])) {\n    # Then fill it with the value of the State cell previous to it\n    h2015$State[i]<-h2015$State[i-1]\n  } \n}\n```\n\nGreat, we've got two data sets with homicides for all cities across the country.\n\nLet's narrow the 2015 data down to the 50 largest cities (Fortunately, the data includes population). And then we'll join it to the 2014 data.\n\nWe'll use the **dplyr** package.\n\n```{r population_filter, warning=F, message=F}\nlibrary(dplyr)\n\n# Arranging by population in descending order\nh2015 <- arrange(h2015, desc(Population))\n\n# Grabbing just the first 50 cities\nh2015_50 <- h2015[1:50,]\n\n# Prepping the two dataframes for joining\nh2015_50 <- select(h2015_50, State, City, Homicides2015=Murder.and.nonnegligent.manslaughter)\nh2014 <- select(h2014, State, City, Homicides2014=Murder.and.nonnegligent.manslaughter)\nh2015_joined <- left_join(h2015_50, h2014, by=c(\"State\", \"City\"))\n```\n\nDid that work?\n\n```{r top5}\nhead(h2015_joined)\n```\n\nHm, it didn't join successfully on some. What's up with that?\n\nLet's take a look at what's going on in New York.\n\n![Aha](images/fbi7.png)\n\nIt looks like a number of the data points have footnote numbers (Dammit, Excel!)\n\nWe have to use regex to clean out those numeric characters.\n\n```{r more_cleaning}\n# This library a function I'll use to trim trailing spaces\nlibrary(stringr)\n\n# These commands get rid of numbers and commas\nh2014$City <- gsub('[0-9]+', '', h2014$City)\nh2014$City <- gsub(',', '', h2014$City)\nh2014$City <- str_trim(h2014$City)\n\nh2014$State <- gsub('[0-9]+', '', h2014$State)\nh2014$State <- gsub(',', '', h2014$State)\nh2014$State <- str_trim(h2014$State)\n\n# Now let's try to bring the 2015 and 2014 dataframes together again\nh2015_joined <- left_join(h2015_50, h2014, by=c(\"State\", \"City\"))\n```\n\nAlright, how many `NA` cells are there now? We can check by typing `sum(is.na(h2015_joined$Homicides2014))` and we'll know there are `r sum(is.na(h2015_joined$Homicides2014))` blank values.\n\nWhy is that?\n\nWell, I won't make you dig around further, but if you did, you'd see that it was because most of those cities weren't in both years.\n\nAs in Fort Worth had data for 2015 but not for 2014.\n\nIn all, Honolulu, Fort Worth, Tucson, and Wichita were not present and thus could not be successfully joined.\n\nNashville was present in both data sets but did not join successfully because in 2015, officials called it \"Nashville Metropolitan\" while in 2014, it was called just \"Nashville\".\n\n**This is the frustrating thing about working with data.** \n\n**It can be incomplete or inconsistent and takes a long time to prepare it before we can analyze it.**\n\nOK, let's do what we can.\n\n\n```{r more_cleaning2s}\n# We can at least fix Nashville quickly\n\nh2014$City <- gsub('Nashville', 'Nashville Metropolitan', h2014$City)\n\n# Now let's try to bring the 2015 and 2014 dataframes together again\nh2015_joined <- left_join(h2015_50, h2014, by=c(\"State\", \"City\"))\n```\n\nAlright, we're down to just four missing cities.\n\nLet's do some math knowing the data is incomplete just for fun.\n\n```{r math1}\ntotal_homicides_2014 <- sum(h2015_joined$Homicides2014, na.rm=T)\ntotal_homicides_2015 <- sum(h2015_joined$Homicides2015, na.rm=T)\n\n# Percent change\n\npercent_change_2015 <- round((total_homicides_2015- total_homicides_2014)/total_homicides_2014*100,2)\n```\n\nSo with four towns missing, there were `r total_homicides_2014` homicides in 2014 and `r total_homicides_2015` homicides in 2015. \n\nThat's a percent change of `r percent_change_2015`, which is close to the 17 percent that Trump alleges. But with homides missing from four towns, the percent change is exaggerated.\n\nLet's try it one more way and try to make it as complete as possible.\n\n```{r more_cleaning3}\n# Let's filter to the largest 50 cities that have both 2014 and 2015 data\n\nh2015 <- select(h2015, State, City, Population, Homicides2015=Murder.and.nonnegligent.manslaughter)\nh2015_joined <- left_join(h2015, h2014, by=c(\"State\", \"City\"))\nh2015_joined <- filter(h2015_joined, !is.na(Homicides2014))\nh2015_joined <- h2015_joined[1:50,]\n```\n\nLet's try the math again.\n\n```{r math2}\ntotal_homicides_2014 <- sum(h2015_joined$Homicides2014, na.rm=T)\ntotal_homicides_2015 <- sum(h2015_joined$Homicides2015, na.rm=T)\n\n# Percent change\n\npercent_change_2015 <- round((total_homicides_2015- total_homicides_2014)/total_homicides_2014*100,2)\n```\n\nAlright, using complete data, the percent change in homicides between 2014 and 2015 is `r percent_change_2015`— which is an increase, but not as high of an increase as Trump alleges.\n\nAlso, what happens when you include all the matched town data and don't limit it to the 50 largest cities? The percent change is 14.34.\n\n# Is that the largest in 25 years?\n\nTo look into this other allegation, we can finally use the FBI's [table-building tool](https://www.ucrdatatool.gov/).\n\n![Navigate to the bottom left of the page](images/fbi8.png)\n\nClick **Multiple agencies, one variable** under the **Large local agencies** section.\n\n![Hold the control button and select **Cities 1,000,000 or over**, **Cities from 500,000 thru 999,999**, and **Cities from 250,000 thru 499,999**](images/fbi9.png)\n\nClick the **Next** button.\n\n![Next](images/fbi10.png)\n\n* Select all the agencies in the menu on the left (command+A or ctrl+A)\n* Choose **Murder and nonnegligent manslaughter** in the pull down menu\n* Pick 1990 to 2014 (which is 24 years total)\n* Click **Get Table**\n\n![Sharp looking table](images/fbi11.png)\n\n\n\n\nOK, get that data by left clicking and saving the link in **Spreadsheet of this table**.\n\nMove that file over to your data folder and load it into R.\n\nLet's take a quick look at the structure before bringing it in.\n\n![Yup, gross structure like last time.](images/fbi12.png)\n\nWhen we bring it in, we'll need to skip the first few rows again.\n\n\n```{r loading_fbi}\ntrends <- read.csv(\"data/LocalCrimeTrendsInOneVar.csv\", stringsAsFactors=F, skip=4)\n```\n\nLooks like that worked except:\n\n![All those footnotes at the bottom are going to mess things up.](images/fbi13.png)\n\nWe can filter that out.\n\n```{r loading_fbi2}\ntrends <- trends[1:82,]\n\n# Also, let's get rid of that floating blank column at the end\ntrends$X <- NULL\n```\n\nLet's figure out the percent change between each year.\n\nWe'll use the **tidyr** package so we can restructure the data so it's easier to figure out the total homicides per year.\n\n```{r wrangling}\nlibrary(tidyr)\n\nre_trends <- trends %>%\n  gather(\"year\", \"homicides\", 3:27) %>%\n  group_by(year) %>%\n  summarize(total=sum(homicides, na.rm=T))\n\nlibrary(knitr)\nkable(re_trends)\n```\n\nAlright, we're getting somewhere.\n\nWe need to clean it up a bit and figure out the percent change year to year, first. \n\nAnd then we can try to visualize it.\n\n```{r year_to_year, fig.width=8, fig.height=5}\nre_trends$year <- gsub(\"X\", \"\", re_trends$year)\n\n# There's probably a faster way to do this, but I'm going to make a loop to determine the percent change for each row.\n\nre_trends$percent_change <- 0\n\nfor (i in 2:nrow(re_trends)) {\n  re_trends$percent_change[i] <- round((re_trends$total[i]-re_trends$total[i-1])/re_trends$total[i-1]*100,2)\n}\n\nlibrary(ggplot2)\n\nggplot(re_trends, aes(x=year, y=percent_change)) +geom_bar(stat=\"identity\")\n```\n\nWell, even though we're not comparing the same cities, we can get a general sense of percent change year to year. \n\nAccording to that measurement, yes, the percent change between 2014 and 2015 of `r percent_change_2015 ` is higher than any other year since 1990.\n\nBut that number is misleading. \n\nWhy?\n\nCheck out the overall trend of raw numbers in these cities.\n\n```{r year_to_year_total, fig.width=8, fig.height=5}\nggplot(re_trends, aes(x=year, y=total)) +geom_bar(stat=\"identity\")\n\n```\n\nAs we can all see, focusing on percent year to year takes away from the larger picture, which is that homicides is on an overall downward trend and has been since the '90s.\n\n# Homicides in Washington DC\n\n\"In our nation’s capital, killings rose by 50 percent over the past four years.\"\n\nIs that true? \n\nAccording to the 2015 data, the number of homicides was 165. Four years earlier, it was 88.\n\nThat's about an 88 percent increase, it's true.\n\nBut once again, why focus on four years?\n\nLet's look at the data.\n\n```{r dc, fig.width=8, fig.height=5}\n\ndc <- filter(trends, State==\"DC\") %>%\n  gather(\"Year\", \"Total\", 3:27) \n\n# Adding the 2015 data\ndc <- rbind(dc, data.frame(Agency=\"Washington Metropolitan Police Dept\", State=\"DC\", Year=\"X2015\", Total=162))\ndc$Year <- gsub(\"X\", \"\", dc$Year)\n\nggplot(dc, aes(x=Year, y=Total)) +geom_bar(stat=\"identity\")\n\n```\nBecause four years ago, DC had the lowest number of homicides in decades. So if you want to exaggerate the change between now and then, you pick the then with lowest point.\n\nIs there an uptick in homcides? Sure. But that's only happened very recently.\n\n# Chicago homicides\n\n\"There were thousands of shootings in Chicago last year alone.\"\n\nYou known what we're going to do.\n\nSame thing as with DC but for Chicago.\n\n```{r chicago, fig.width=8, fig.height=5}\nchi <- filter(trends, Agency==\"Chicago Police Dept\") %>%\n  gather(\"Year\", \"Total\", 3:27) \n\n# Adding the 2015 data\nchi <- rbind(chi, data.frame(Agency=\"Chicago Police Dept\", State=\"IL\", Year=\"X2015\", Total=478))\nchi$Year <- gsub(\"X\", \"\", chi$Year)\n\nggplot(chi, aes(x=Year, y=Total)) +geom_bar(stat=\"identity\")\n```\n\nThe FBI doesn't track shootings, only homicides.\n\nWe'd need to get that data from the Chicago Police themselves, most likely.\n\nSo, it's true that shootings are in the [\"thousands\"](http://crime.chicagotribune.com/chicago/shootings/) but in relation to what? \n\nWe know for sure that deaths are going down in Chicago. It's most likely that shootings have gone down, too.\n\nWe'll only know for sure if we have the data.\n\nAlso, why the fixation on Chicago? Is that fair? Sure they have a lot of shootings and deaths, but the city is also a large population.\n\nLet's check that real quick with the 2015 data since that data set includes both total and population.\n\n```{r per_capita}\nlibrary(DT)\n\nh2015b <- select(h2015, State, City, Population, Homicides2015) %>%\n  filter(!is.na(City)) %>%\n  filter(Population>100000)\n\nh2015b$per_capita <- round(h2015b$Homicides2015/h2015b$Population*100000,2)\n\nh2015b <- arrange(h2015b, desc(per_capita))\ndatatable(h2015b)\n\n```\nSt. Louis and Baltimore and many other cities actually have higher homicides per 100,000 residents than Chicago.\n\nChicago's actually 25th compared to all the other cities. Even Hartford has a higher rate of homicides.\n\nWhat's the lesson in all this?\n\nIt's not that difficult to verify claims when the data is easily available. \n\nThe hardest part was just cleaning up the data to get apples-to-apples data.\n",
    "created" : 1485708000885.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2649458114",
    "id" : "2DC1080D",
    "lastKnownWriteTime" : 1485722140,
    "last_content_update" : 1485722140860,
    "path" : "~/Documents/Github/wesleyan-case-studies/crime-data/homicides.Rmd",
    "project_path" : "homicides.Rmd",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}